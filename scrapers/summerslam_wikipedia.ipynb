{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7366ccd4-f7e2-4771-9e56-72b8853a4b1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 39 SummerSlam event pages...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing events: 100%|███████████████████████████████████████████████████████████████| 39/39 [00:10<00:00,  3.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE! See file: summerslam_infobox_master.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# infobox scraper\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "WIKI_BASE = \"https://en.wikipedia.org\"\n",
    "CATEGORY_URL = f\"{WIKI_BASE}/wiki/Category:SummerSlam\"\n",
    "\n",
    "def get_event_links():\n",
    "    resp = requests.get(CATEGORY_URL)\n",
    "    resp.raise_for_status()\n",
    "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "    links = []\n",
    "    for a in soup.select('.mw-category a[href*=\"SummerSlam_\"]'):\n",
    "        href = a['href']\n",
    "        title = a.get_text(strip=True)\n",
    "        if '(' in title and ')' in title and 'SummerSlam' in title:\n",
    "            links.append((title, WIKI_BASE + href))\n",
    "    links = list(set(links))\n",
    "    links.sort(key=lambda x: x[0])\n",
    "    return links\n",
    "\n",
    "def get_infobox(soup):\n",
    "    infobox = soup.find('table', {'class': lambda x: x and 'infobox' in x})\n",
    "    data = {}\n",
    "    if infobox:\n",
    "        for row in infobox.find_all('tr'):\n",
    "            if row.th and row.td:\n",
    "                key = row.th.get_text(\" \", strip=True)\n",
    "                val = row.td.get_text(\" \", strip=True)\n",
    "                data[key] = val\n",
    "    return data\n",
    "\n",
    "def merge_tagline(box):\n",
    "    # Collapse any \"Tagline(s)\", \"Tagline (s)\", etc. into 'Tagline' (single column)\n",
    "    tagline_keys = [k for k in box if re.match(r'^tagline(\\s*\\(\\s*s\\s*\\))?$', k.lower().replace('\\xa0', '')) or\n",
    "                                   re.match(r'^tagline\\s*\\(\\s*s\\s*\\)$', k.lower().replace('\\xa0', ''))]\n",
    "    main_tagline = box.get(\"Tagline\", \"\").strip() if \"Tagline\" in box else \"\"\n",
    "    other_tagline = \"\"\n",
    "    for k in tagline_keys:\n",
    "        if k != \"Tagline\" and box.get(k, \"\").strip():\n",
    "            other_tagline = box[k].strip()\n",
    "            break  # take first non-empty\n",
    "    for k in tagline_keys:\n",
    "        box.pop(k, None)\n",
    "    box[\"Tagline\"] = main_tagline if main_tagline else other_tagline\n",
    "    return box\n",
    "\n",
    "def relabel_keys(box):\n",
    "    \"\"\"\n",
    "    Relabel \"Brand(s)\" and close variants to \"brand\",\n",
    "    and \"Buy rate\" (case-insensitive, with or without space) to \"buy_rate\".\n",
    "    \"\"\"\n",
    "    # Normalize key mapping: key in lower, spaces removed for matching\n",
    "    new_box = {}\n",
    "    for key, value in box.items():\n",
    "        norm = key.lower().replace(' ', '').replace('\\xa0','')\n",
    "        # Brand(s) or Brand (s) variants\n",
    "        if norm in ['brand(s)', 'brand(s)', 'brands', 'brand(s)']:\n",
    "            new_box[\"brand\"] = value\n",
    "        # Buy rate, any case/spacing\n",
    "        elif norm in ['buyrate', 'buy rate', 'buy rate']:\n",
    "            new_box[\"buy_rate\"] = value\n",
    "        else:\n",
    "            new_box[key] = value  # Keep original\n",
    "    # In case a dict ends up with both original and relabeled, remove dupes\n",
    "    for old_key in list(new_box.keys()):\n",
    "        norm = old_key.lower().replace(' ', '').replace('\\xa0', '')\n",
    "        if (norm in ['brand(s)', 'brands']) or (norm in ['buyrate', 'buy rate', 'buy rate']):\n",
    "            if old_key not in [\"brand\", \"buy_rate\"]:\n",
    "                new_box.pop(old_key, None)\n",
    "    return new_box\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    event_links = get_event_links()\n",
    "    all_infoboxes = []\n",
    "\n",
    "    print(f\"Found {len(event_links)} SummerSlam event pages...\")\n",
    "\n",
    "    for event_title, event_url in tqdm(event_links, desc=\"Processing events\"):\n",
    "        try:\n",
    "            resp = requests.get(event_url)\n",
    "            resp.raise_for_status()\n",
    "            soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "            infobox = get_infobox(soup)\n",
    "            infobox[\"Event\"] = event_title\n",
    "            infobox = merge_tagline(infobox)\n",
    "            infobox = relabel_keys(infobox)\n",
    "            all_infoboxes.append(infobox)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {event_title}: {e}\")\n",
    "\n",
    "    # Collect fields after taglines merged and keys relabeled\n",
    "    all_fields = set()\n",
    "    for box in all_infoboxes:\n",
    "        all_fields.update(box.keys())\n",
    "    # Remove any \"Tagline(s)\" variant and ensure preferred order\n",
    "    all_fields = {f for f in all_fields if not re.match(r'^tagline\\s*\\(\\s*s\\s*\\)$', f.lower().replace('\\xa0',''))}\n",
    "    # Event and Tagline up front, then brand and buy_rate if present, then rest\n",
    "    field_order = [\"Event\", \"Tagline\"]\n",
    "    if \"brand\" in all_fields:\n",
    "        field_order.append(\"brand\")\n",
    "    if \"buy_rate\" in all_fields:\n",
    "        field_order.append(\"buy_rate\")\n",
    "    # Add any remaining fields\n",
    "    field_order += sorted(f for f in all_fields if f not in field_order)\n",
    "    df = pd.DataFrame([{f: box.get(f, \"\") for f in field_order} for box in all_infoboxes])\n",
    "    df = df[field_order]  # ensure order\n",
    "    df.sort_values(\"Event\", inplace=True)\n",
    "    df.to_csv(\"summerslam_infobox_master.csv\", index=False)\n",
    "\n",
    "    print(\"DONE! See file: summerslam_infobox_master.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d533ba51-bb57-4ac4-859e-701b7a53cd3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 39 SummerSlam event pages...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing events: 100%|███████████████████████████████████████████████████████████████| 39/39 [00:14<00:00,  2.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 307 matches from 37 events.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# results scraper\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup, Tag\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "WIKI_BASE = \"https://en.wikipedia.org\"\n",
    "CATEGORY_URL = f\"{WIKI_BASE}/wiki/Category:SummerSlam\"\n",
    "DESIRED_COLS = [\"No.\", \"Results\", \"Stipulations\", \"Times\"]\n",
    "\n",
    "def clean_header(header):\n",
    "    header = re.sub(r\"\\[.*?\\]\", \"\", header.replace('\\xa0', ' ')).strip()\n",
    "    header = re.sub(r\"\\s+\", \" \", header)\n",
    "    return header.lower()\n",
    "\n",
    "def canonical_name(header):\n",
    "    h = clean_header(header)\n",
    "    if h in {\"no.\", \"no\"}: return \"No.\"\n",
    "    if \"result\" in h:     return \"Results\"\n",
    "    if h.startswith(\"stipul\"): return \"Stipulations\"\n",
    "    if h.startswith(\"time\"):   return \"Times\"\n",
    "    return None\n",
    "\n",
    "def get_event_links():\n",
    "    resp = requests.get(CATEGORY_URL)\n",
    "    resp.raise_for_status()\n",
    "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "    links = []\n",
    "    for a in soup.select('.mw-category a[href*=\"SummerSlam_\"]'):\n",
    "        href = a['href']\n",
    "        title = a.get_text(strip=True)\n",
    "        if '(' in title and ')' in title and 'SummerSlam' in title:\n",
    "            links.append((title, WIKI_BASE + href))\n",
    "    links = list(set(links))\n",
    "    def year_key(x):\n",
    "        m = re.search(r\"(\\d{4})\", x[0])\n",
    "        return int(m.group(1)) if m else 0\n",
    "    links.sort(key=year_key)\n",
    "    return links\n",
    "\n",
    "def find_results_table_1990(soup):\n",
    "    \"\"\"Find the FIRST wikitable after the 'Results' heading. Returns table or None.\"\"\"\n",
    "    results_header = soup.find(lambda tag: tag.name in ['h2','h3'] and 'Results' in tag.get_text())\n",
    "    if not results_header:\n",
    "        return None\n",
    "    # Find the first 'wikitable' after the Results heading\n",
    "    next_tag = results_header\n",
    "    for _ in range(15):\n",
    "        next_tag = next_tag.find_next()\n",
    "        if not next_tag:\n",
    "            return None\n",
    "        if isinstance(next_tag, Tag):\n",
    "            if next_tag.name == \"table\" and \"wikitable\" in (next_tag.get('class') or []):\n",
    "                return next_tag\n",
    "    return None\n",
    "\n",
    "def find_results_table(soup, year):\n",
    "    # For 1990: specifically use find_results_table_1990\n",
    "    if year == \"1990\":\n",
    "        table = find_results_table_1990(soup)\n",
    "        if table:\n",
    "            return table\n",
    "    # For others: use the general finder (looks for a wikitable with right headers)\n",
    "    for table in soup.find_all(\"table\", class_=\"wikitable\"):\n",
    "        headers = [th.get_text(\" \", strip=True) for th in table.find_all(\"th\")]\n",
    "        found = [canonical_name(h) for h in headers]\n",
    "        if \"No.\" in found and \"Results\" in found:\n",
    "            return table\n",
    "    return None\n",
    "\n",
    "def table_to_rows(table):\n",
    "    tr_iter = table.find_all('tr')\n",
    "    if not tr_iter:\n",
    "        return []\n",
    "    header_cells = [th.get_text(' ', strip=True) for th in tr_iter[0].find_all(['th', 'td'])]\n",
    "    canon_map = [canonical_name(h) for h in header_cells]\n",
    "    rows = []\n",
    "    for tr in tr_iter[1:]:\n",
    "        tds = tr.find_all(['td', 'th'])\n",
    "        if not tds or all(not td.get_text(strip=True) for td in tds):\n",
    "            continue\n",
    "        data = {col: \"\" for col in DESIRED_COLS}\n",
    "        for idx, td in enumerate(tds):\n",
    "            if idx < len(canon_map) and canon_map[idx]:\n",
    "                data[canon_map[idx]] = td.get_text(\" \", strip=True)\n",
    "        # Only keep rows where 'No.' is present and is numeric\n",
    "        no_clean = data[\"No.\"].replace('\\xa0', '').strip()\n",
    "        if no_clean.isdigit():\n",
    "            data[\"No.\"] = no_clean\n",
    "            rows.append(data)\n",
    "    return rows\n",
    "\n",
    "def extract_results_from_list(soup):\n",
    "    # Fallback logic for future years with a <ul> under a Results header\n",
    "    rows = []\n",
    "    results_header = soup.find(lambda tag: tag.name in ['h2','h3'] and 'Results' in tag.get_text())\n",
    "    if not results_header:\n",
    "        return []\n",
    "    ul = results_header.find_next_sibling(lambda tag: tag.name == \"ul\" and tag.find(\"li\"))\n",
    "    if not ul:\n",
    "        return []\n",
    "    for i, li in enumerate(ul.find_all(\"li\"), 1):\n",
    "        text = li.get_text(\" \", strip=True)\n",
    "        row = {\n",
    "            \"No.\": str(i),\n",
    "            \"Results\": text,\n",
    "            \"Stipulations\": \"\",\n",
    "            \"Times\": \"\",\n",
    "        }\n",
    "        rows.append(row)\n",
    "    return rows\n",
    "\n",
    "def get_event_name(soup, default_name):\n",
    "    infobox = soup.find(\"table\", class_=\"infobox\")\n",
    "    if infobox:\n",
    "        caption = infobox.find(\"caption\")\n",
    "        if caption and caption.get_text(strip=True):\n",
    "            return caption.get_text(strip=True)\n",
    "    return default_name\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    event_links = get_event_links()\n",
    "    all_rows = []\n",
    "    print(f\"Found {len(event_links)} SummerSlam event pages...\")\n",
    "\n",
    "    for event_title, event_url in tqdm(event_links, desc=\"Processing events\"):\n",
    "        try:\n",
    "            resp = requests.get(event_url)\n",
    "            resp.raise_for_status()\n",
    "            soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "            event_name = get_event_name(soup, event_title)\n",
    "            m = re.search(r\"(\\d{4})\", event_name)\n",
    "            event_year = m.group(1) if m else None\n",
    "\n",
    "            table = find_results_table(soup, event_year)\n",
    "            if table:\n",
    "                rows = table_to_rows(table)\n",
    "            else:\n",
    "                rows = extract_results_from_list(soup)\n",
    "            for row in rows:\n",
    "                row[\"Event\"] = event_name\n",
    "            all_rows.extend(rows)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {event_title}: {e}\")\n",
    "\n",
    "    out_order = [\"Event\"] + DESIRED_COLS\n",
    "    df = pd.DataFrame(all_rows)\n",
    "    for col in out_order:\n",
    "        if col not in df.columns:\n",
    "            df[col] = \"\"\n",
    "    df = df[out_order]\n",
    "    df.to_csv(\"summerslam_results_master.csv\", index=False)\n",
    "    print(f\"Wrote {len(df)} matches from {len(set(df['Event']))} events.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f097cf-af5b-4093-b949-984f5b53be8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
